---
title: "Schedule"
---

**Click on the text like "Week 1: Jan 22 -- 26" to expand or collapse the items we covered in that week.**

I will fill in more detail and provide links to lecture notes and labs as we go along.  Items for future dates are tentative and subject to change.

```{r, echo = FALSE}
make_week_box <- function(id, open, title, contents) {
  cat('
<div class="panel panel-default">
<div class="panel-heading" role="tab" id="headingOne">
<h5 class="mb-0">
<a data-toggle="collapse" href="#',
id,
'" aria-expanded="true" aria-controls="collapseOne">
',
title,
'</a>
</h5>
</div>

<div id="',
id,
'" class="collapse',
ifelse(open, " in", ""),
'">
<div class="panel-body">
',
contents,
'
</div>
</div>
</div>',
  sep = "")
}
```

```{r, echo = FALSE, results='asis'}
make_week_box("week1", FALSE, "Week 1: Jan 22 -- 26",
"
* Maximum Likelihood Estimation and Bayesian Inference in Analytically Tractable Models

#### Tue, Jan 23

 * **In class**, we will work on:
    * Introduction to Maximum Likelihood Estimation: capture-recapture [pdf](lectures/20180123_mle/20180123_capture_recapture.pdf)
    * Other materials: Summary of common discrete distributions [pdf](lectures/20180123_mle/20180123_discrete_distributions_summary.pdf)

#### Thu, Jan 25

 * **Before class**, please do the following:
    * **Fill out** a brief [questionnairre](https://goo.gl/forms/vIsi277yi74DrswL2)
    * **Fill out** this poll about when my office hours should be held: [http://whenisgood.net/5h5dzyy](http://whenisgood.net/5h5dzyy)
    * **Sign up** for our class at gryd.us (the site where we will use R starting Thursday!): [https://beta.gryd.us/course/pASBfTReABjoxzuzQv2g2J/register/](https://beta.gryd.us/course/pASBfTReABjoxzuzQv2g2J/register/)
    * **Sign up** for our class at Piazza (anonymous question and answer forum): [https://piazza.com/mtholyoke/spring2018/stat343](https://piazza.com/mtholyoke/spring2018/stat343)
    * **Reading**
      * Section 7.1: just skim this section, but do pay attention to Definitions 7.1.1, 7.1.2, 7.1.3, and 7.1.4.
      * Section 7.5: Pay attention to Definitions 7.5.1 and 7.5.2. Work through one or two of the examples.
      * Sections 7.2 and 7.3: Read somewhat carefully, but we will cover this in class today.
 * **In class**, we will work on:
    * Example of Maximum Likelihood Estimation: friend or foe [pdf](lectures/20180125_mle_continued/20180125_mle_friend_or_foe.pdf)
    * If time: Example of Maximum Likelihood Estimation: seedlings [pdf](lectures/20180123_mle/20180123_seedlings.pdf)  Solutions: [pdf](lectures/20180123_mle/20180123_seedlings_sol.pdf)
    * State some definitions related to MLE more carefully: Definitions 7.2.3/7.5.1 (it's the same definition repeated to emphasize its importance), 7.4.1, and 7.5.2.
    * Introduction to the idea of subjective probabilities
 * **After class**, please do the following:
    * Review material from today's class.  Post on Piazza or visit my office if you have any questions!
    * If you didn't have time to look at the seedlings example of MLE, I recommend you take a look at it; ask if you have questions!

#### Fri, Jan 26
 * **Before Class**, please do the following:
    * **Reading**
      * Finish a first pass at reading Sections 7.1, 7.2, 7.3, and 7.5 of DeGroot and Schervish
 * **In Class**, we will work on:
    * Finish our introduction to maximum likelihood estimation (we will talk about this much more in future weeks)
    * Introduction to the idea of subjective probability distributions
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("week2", FALSE, "Week 2: Jan 29 -- Feb 2",
"
Bayes Estimates; start on Computational Issues

#### Tue, Jan 30
 * **Before Class**, please do the following:
    * **Register for Github** and send me an email with your user id: eray@mtholyoke.edu
    * **Do Beta distribution calibration exercise on Gryd**.  Sign into Gryd.  Click 'My Courses' at the top.  Click `Beta Distribution`.  Work through the exercise.
 * **In Class**, we will do the following:
    * Discuss Bayesian estimation for a Binomial model
    * Intro to Git and GitHub
      * Link for today's lab: https://classroom.github.com/a/j23r_ajm
      * Resource: [Git workflow (pdf)](resources/git_workflow/git_workflow.pdf)
      * Resource: [Git cheatsheet (pdf)](resources/git-cheat-sheet-education.pdf)
      * Resource: [Basic commands for Unix navigation (pdf)](resources/unix_navigation.pdf)
    * Comparison of MLE and Bayesian estimates for a Binomial model: [pdf](lectures/20180130_bayes_binomial/20180130_bayes_binomial.pdf)

#### Thu, Feb 1
 * **Before Class**, please do the following:
    * Have a good morning!
 * **In Class**, we will do the following:
    * Lab about inference for a binomial.
    * More about Git.

#### Fri, Feb 2
 * **Before Class**, please do the following:
    * **Reading** Section 7.4 of DeGroot and Schervish
 * **In Class**, we will do the following:
    * Wrap up comparison of MLE and Bayesian inference for a binomial
    * More about Git.
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("week3", FALSE, "Week 3: Feb 5 -- 9",
"

#### Tue, Feb 6
 * **Before Class**, please do the following:
    * If you were unable to get the first lab committed to github, take another look at that over the weekend (but don't spend too much time on this).  If you're still stuck on Monday, let me know and we can work through it together.
    * Try cloning the first problem set (PS1) from GitHub to Gryd.  The link to the assignment on GitHub is on the homework page.
    * Start on PS1.
 * **In Class**, we will do the following:
    * Start thinking about sampling distributions; central limit theorem.  Ideas from Sections 6.3 and 8.1 of the book.
    * Overview of ggplot2: [pdf](lectures/20180206_ggplot2/20180202_visualization.pdf)

#### Thu, Feb 8
 * **In class**, we will do the following:
    * Start a lab about the central limit theorem; Solutions posted on Thursday, Feb. 15.

#### Fri, Feb 9
 * **In class**, we will do the following:
    * Maximum likelihood estimation for the parameters of a univariate normal distribution
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("week4", FALSE, "Week 4: Feb 12 -- 16",
"

#### Tue, Feb 13
 * **Before Class**, please do the following:
    * **PS2** is due today!
 * **In class**, we will do the following:
    * Start Bayesian estimation for the mean of a univariate normal distribution, assuming the variance is known

#### Thu, Feb 15
 * **Before Class**, please do the following:
    * **PS1** is due today!
 * **In class**, we will do the following:
    * Wrap up Bayesian estimation for the mean of a univariate normal distribution, assuming the variance is known
    * Wrap up Central Limit Theorem lab we started on Feb 8.
        * [Solutions on GitHub](https://github.com/mhc-stat343-s2018/lab-clt-solutions) - you could clone from here
        * [Notebook viewer](http://nbviewer.jupyter.org/urls/cdn.rawgit.com/mhc-stat343-s2018/lab-clt-solutions/b82c7724/lab_clt.ipynb) - to just look at what I wrote up

#### Fri, Feb 16
 * **In class**, we will do the following:
    * Start on normal approximation to the sampling distribution of the MLE.
        * [Example from class on GitHub](https://github.com/mhc-stat343-s2018/lecture-mle-sampling-dist-normal-approx) - you could clone from here
        * [Notebook viewer](https://nbviewer.jupyter.org/urls/cdn.rawgit.com/mhc-stat343-s2018/lecture-mle-sampling-dist-normal-approx/b4cddf67/exponential_loglik_plots.ipynb) - to just look at example from class
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("week5", FALSE, "Week 5: Feb 19 -- 23",
"
#### Tue, Feb 20
 * **In class**, we will do the following:
    * Discuss Bias, Variance, and Mean Squared Error (MSE) of estimators.  In Schervish, this is covered in Section 8.7, and the first couple of pages of Section 7.9.  In Rice, this is not covered particularly systematically anywhere; the closest we can get is in Section 7.3.1 and Section 4.2.1.
    * I added example calculations of bias, variance, and MSE based on simulation-based approximation to sampling distribuiton to the example from last class (see notebook viewer)
    * Start a lab using these ideas; you have been added as a collaborator on a GitHub repository.

#### Thu, Feb 22
 * Continue lab

#### Fri, Feb 23
 * Finish lab
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("week6", FALSE, "Week 6: Feb 26 -- Mar 2",
"

#### Tue, Feb 27
 * Newton's Method and Optimization

#### Thu, Mar 1
 * Newton's Method and Optimization
    * Walk through code: [pdf](lectures/20180301_optimization_notes/20180301_optimization_notes.pdf)
    * Lab on Gryd; fitting a Weibull

#### Fri, Mar 2
 * Newton's Method and Optimization
    * Recap: slides [pdf](lectures/20180302_optimization_notes_2/20180302_NR_slides.pdf)
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("week7", TRUE, "Week 7: Mar 5 -- 9",
"
**Midterm 1; covers material from PS1 and PS2**

 * **Readings for the next couple of weeks**:
    * The Law of Large Numbers and Monte Carlo Integration: Section 5.2 of Rice has a good introduction to the ideas; see Theorem A and Example A [here](resources/readings/rice_mc_integration.pdf).  Related material is also in Section 6.2 in DeGroot and Schervish, especially Theorems 6.2.4 through 6.2.6.
    * Sections about Bayesian Inference and MCMC from \"Introduction to Statistical Thought\" by Lavine: [pdf](resources/readings/lavine_bayes_mcmc.pdf).  The models used in that text are more complicated than the models we're using in the class at this point, but this is the best introductory discussion of MCMC I could find.  In case you're interested, the full text is available [here](http://people.math.umass.edu/~lavine/Book/book.html).

#### Tue, Mar 6
 * **Before Class**, please do the following:
    * **PS2** is due today!
 * **In Class**, we will do the following:
    * Introduction to Monte Carlo methods for Bayesian inference: [pdf](lectures/20180306_mcmc/20180306_mcmc.pdf)
    * Watch this video: https://www.youtube.com/watch?v=OTO1DygELpY
    * Start a lab about these ideas; you have been added as a collaborator on a repository called `lab-intro-mcmc-yourname` on GitHub

#### Thu, Mar 8

#### Fri, Mar 9
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("week8", TRUE, "Week 8: Mar 12 -- 16",
"
#### Spring Break: safe travels!
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("week9", TRUE, "Week 9: Mar 19 -- 23",
"

#### Tue, Mar 20
 * **In Class**, we will do the following:
    * Attempt to install Jupyter on peoples' laptops.... :(

#### Thu, Mar 22
 * **In Class**, we will do the following:
    * Talk about frequentist confidence intervals based on the large-sample normal approximation to the sampling distribution of the maximum likelihood estimator.  [pdf](lectures/20180322_large_sample_ci/20180322_large_sample_ci.pdf)
    * This is in Section 8.5 of Rice, see Examples B and C and the discussion immediately preceeding them

#### Fri, Mar 23
 * **In Class**, we will do the following:
    * Continuing from last time, talk about Bayesian credible intervals based on the large-sample normal approximation to the posterior distribution.
    * A quick look at Bayesian interval estimation via MCMC.  Lab posted on GitHub, called lab-stan-poisson.
    * This is not really discussed in our books, unfortunately.  A similar idea is discussed in Rice Section 8.6, but the intervals are obtained in a setting where a conjugate prior is used, not when large-sample approximations or MCMC methods must be used - see Example A on pages 288 through 290.  Examples B and C also present Bayesian credible intervals with less discussion.  The section of \"Introduction to Statistical Thought\" by Michael Lavine posted on the resources page of the course website does discuss interval estimation via MCMC as well.
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("week10", TRUE, "Week 10: Mar 26 -- 30",
"

#### Tue, Mar 27
 * **In Class**, we will do the following:
    * No new material.
    * Time to practice the mechanics of interval estimation using the three approaches discussed so far: large-sample frequentist confidence intervals, large-sample Bayesian credible intervals, and Bayesian credible intervals via MCMC.

#### Thu, Mar 29
 * **In Class**, we will do the following:
    * Start thinking about bootstrap confidence intervals.


#### Fri, Mar 30
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("week11", TRUE, "Week 11: Apr 2 -- 6",
"

#### Tue, Apr 3

#### Thu, Apr 5

#### Fri, Apr 6
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("week12", TRUE, "Week 12: Apr 9 -- 13",
"

#### Tue, Apr 10

#### Thu, Apr 12

#### Fri, Apr 13
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("week13", TRUE, "Week 13: Apr 16 -- 20",
"

#### Tue, Apr 17

#### Thu, Apr 19

#### Fri, Apr 20
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("week14", TRUE, "Week 14: Apr 23 -- 27",
"

#### Tue, Apr 24

#### Thu, Apr 26

#### Fri, Apr 27
"
  )
```

```{r, echo = FALSE, results='asis'}
make_week_box("finals", TRUE, "Final Exams: Thu, May 3 at 7pm -- Mon, May 7 at noon",
"We will have a cumulative final exam.
"
  )
```
